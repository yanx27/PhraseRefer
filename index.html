<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>PhraseRefer</title>    
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
    <!-- <link href="https://apps.bdimg.com/libs/bootstrap/3.2.0/css/bootstrap.min.css" rel="stylesheet"> -->
    <link href="./css/bootstrap.min.css" rel="stylesheet">
    <link href="./css/ghotish.css" rel="stylesheet">
    <meta content="Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phrases" name="description">
    <meta content="Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phrases" property="og:title">
    <meta content="Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phrases." property="og:description">
    <meta content="http://people.eecs.berkeley.edu/~tancik/nerf/website_renders/images/nerf_graph.jpg" property="og:image">
    <meta content="Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phrases" property="twitter:title">
    <meta content="Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phrases." property="twitter:description">
    <meta content="http://people.eecs.berkeley.edu/~tancik/nerf/website_renders/images/nerf_graph.jpg" property="twitter:image">
    <meta property="og:type" content="website">
    <meta content="summary_large_image" name="twitter:card">
    <meta content="width=device-width, initial-scale=1" name="viewport">

</head>

<body>
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <h1 class="text-center d-block text-dark pt-5">Toward Explainable and Fine-Grained 3D Grounding <br>through Referring Textual Phrases</h1>
                <p class="text-center d-block text-dark"><a class="text-secondary" href="https://github.com/CurryYuan">Zhihao Yuan*,</a> 
                    <a class="text-secondary" href="https://yanx27.github.io/">Xu Yan*,</a> Zhuo Li*, Xuhao Li
                    Yao Guo, Shuguang Cui, <a class="text-secondary" href="https://mypage.cuhk.edu.cn/academics/lizhen/"> Zhen Li</a></p>
                <p class="text-center d-block text-dark">The Chinese University of Hong Kong, Shenzhen </p>
            </div>

            <div class="row gx-5 justify-content-center row-cols-3">
                <div class="col-1 p-3 text-decoration-none mx-5">
                    <button type="button" class="btn"><a class="d-block mx-auto" href="https://arxiv.org/abs/2207.01821"><img src="./figs/book.svg" width="100%"/>
                    </a>
                    <p style="padding-top: 10px;">Paper</p></button>
                </div>
                <div class="col-1 p-3 text-decoration-none mx-5">
                    <button type="button" class="btn">
                        <a class="d-block mx-auto" href="https://github.com/CurryYuan/PhraseRefer" title="Coming soon"><img src="./figs/code.svg" width="100%"/>
                    </a>
                    <p style="padding-top: 17px;"><a title="Coming soon">Code</a></p></button>
                </div>
                <div class="col-1 p-3 text-decoration-none mx-5">
                    <button type="button" class="btn">
                        <a class="d-block mx-auto" href="" title="Coming soon"><img src="./figs/box.svg" width="100%"/>
                    </a>
                    <p style="padding-top: 5px;"><a title="Coming soon">Dataset</a></p></button>
                </div>
            </div>

            <div class="row gx-5 justify-content-center row-cols-1">
                <h3 style="text-align: center;"><div class="intro" >Abstract</div></h3>
                <div class="col-9 p-3 text-decoration-none mx-5">
                    <p style="text-align: center;">Recent progress on 3D scene understanding has explored visual grounding (3DVG) to localize a target object through a language description. However, existing methods only consider the dependency between the entire sentence and the target object, thus ignoring fine-grained relationships between contexts and non-target ones. In this paper, we extend 3DVG to a more reliable and explainable task, called <b style="color: black;">3D Phrase Aware Grounding (3DPAG)</b>. The 3DPAG task aims to localize the target object in the 3D scenes by explicitly identifying all phrase-related objects and then conducting reasoning according to contextual phrases. To tackle this problem, we label about 400K phrase-level annotations from 170K sentences in available 3DVG datasets, i.e., Nr3D, Sr3D and ScanRefer. By tapping on these developed datasets, we propose <b style="color: black;">a novel framework, i.e., PhraseRefer</b>, which conducts phrase-aware and object-level representation learning through phrase-object alignment optimization as well as phrase-specific pre-training. In our setting, we extend previous 3DVG methods to the phrase-aware scenario and provide metrics to measure <b style="color: black;">the explainability of the 3DPAG task</b>. Extensive results confirm that 3DPAG effectively boosts the 3DVG, and PhraseRefer achieves <b style="color: black;">state-of-the-arts</b> across three datasets, i.e., 63.0%, 54.4% and 55.5% overall accuracy on Sr3D, Nr3D and ScanRefer, respectively.</p>
                </div>
            </div>
            
            <div class="row gx-5 justify-content-center row-cols-1" style="padding-top: 50px;">
                <h3 style="text-align: center;"><div class="intro" >Overview</div></h3>
                <div class="col-7 p-3 text-decoration-none mx-5">
                    <a class="d-block mx-auto"><img src="./figs/figure1.png" width="100%"/>
                        </a>
                        <p style="text-align: center;">Compared with previous 3DVG (a) only grounding the sole target object, 3DPAG (b) requires <b style="color: black;">the neural listener to identify all phrase-aware objects (target and non-target) in the 3D scene </b>and then <b style="color: black;"></b>explicitly conduct reasoning </b>over all objects through the contexts. Note that the 3D bounding boxes are annotated using the same color with the corresponding object phrases in the sentence. Best viewed in colored.
                        </p>
                </div>
            </div>
            
            <div class="row gx-5 justify-content-center row-cols-1" style="padding-top: 50px;">
                <h3 style="text-align: center;"><div class="intro">Framework</div></h3>
                <div class="col-7 p-3 text-decoration-none mx-5">
                    <a class="d-block mx-auto"><img src="./figs/figure2.png" width="100%"/>
                        </a>
                        <p style="text-align: center;"><b style="color: black;">Cross-modal Transformer with Phrase-Object Alignment (POA) Optimization.</b> Part (a) illustrates our baseline model, i.e., cross-modal transformer, and part (b) shows the process of optimizing the phrase-object alignment map.</p>
                </div>
                <div class="col-7 p-3 text-decoration-none mx-5">
                    <a class="d-block mx-auto"><img src="./figs/figure3.png" width="100%"/>
                        </a>
                        <p style="text-align: center;"><b style="color: black;">Phrase-Specific Pre-training.</b> In the phrase-specific pre-training stage, we generate the phrase-specific masks according to the grounding truth phrases, e.g., setting the position of “the office chair” to 1 and other positions to 0. Then, we design the network to predict the corresponding object of the selected phrase. During the fine-tuning stage, we only predict the target object referred by the whole sentence.</p>
                </div>
            </div>

            <div class="row gx-5 justify-content-center row-cols-1" style="padding-top: 50px;">
                <h3 style="text-align: center;"><div class="intro" >Visualization Results</div></h3>
                <div class="col-7 p-3 text-decoration-none mx-5">
                    <a class="d-block mx-auto"><img src="./figs/figure4.png" width="100%"/>
                        </a>
                        <p style="text-align: center;"><b style="color: black;">Visualization Results of 3DVG.</b> We visualize the visual grounding results of
                            SAT and ours. The left four examples are our correct predictions while SAT is not. The
                            right two examples show the representative failures for both current SOTA model SAT
                            and our PhraseRefer. The green/red/blue colors illustrate the correct/incorrect/GT
                            boxes. The target class for each query is shown in red color. We provide rendered
                            scenes in the first row for better visualization. Best viewed in color.
                        </p>
                </div>
                <div class="col-7 p-3 text-decoration-none mx-5">
                    <a class="d-block mx-auto"><img src="./figs/figure5.png" width="100%"/>
                        </a>
                        <p style="text-align: center;"><b style="color: black;">Visualization Results of 3DPAG.</b> We show examples of 3DPAG prediction
                            and POA map of our method.The corresponding phrase and bounding box are drawn
                            in the same color. Best viewed in color.
                        </p>
                </div>
            </div>
        </div>
    <div>

    <div class="container-sm bg-light p-4 w-75">
        <h3 class="subtitle">Citation</h3>
        <pre><code>
	@article{yuan2022toward,
	  title={Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phrases},
	  author={Yuan, Zhihao and Yan, Xu and Li, Zhuo and Li, Xuhao and Guo, Yao and Cui, Shuguang and Li, Zhen},
	  journal={arXiv preprint arXiv:2207.01821},
	  year={2022}
	}
        </code></pre>
    </div>

</body>

</html>